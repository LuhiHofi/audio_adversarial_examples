# THIS YAML FILE WAS CREATED BY Lukáš Hofman with the help of univ_lang_fit.yaml file from whisper_attack repository
# -------------------------------------------------------------

# General information
seed: 1002
__set_seed: !apply:torch.manual_seed [!ref <seed>]
root: !PLACEHOLDER
tokenizers_folder: !ref <root>/tokenizers

# Attack information
eps: 0.01
eps_item: 0.002
nb_iter: 200
rel_eps_iter: 0.01
success_every: 100
background_audio_path: <root>/audio_backgrounds/raining-in-backyard.wav
t: 0.8
background_noise_strength: 0.5
delta: !new:robust_speech.adversarial.utils.TensorModule
   size: (288000,)
epochs: 10000
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <epochs>

attack_class: !name:universal_with_noise.UniversalWithNoiseWhisperPGDAttack
  eps: !ref <eps>
  nb_iter: !ref <nb_iter>
  rel_eps_iter: !ref <rel_eps_iter>
  success_every: !ref <success_every>
  eps_item: !ref <eps_item>
  univ_perturb: !ref <delta>
  epoch_counter: !ref <epoch_counter>
  background_audio_path: !ref <background_audio_path>
  t: !ref <t>
  background_noise_strength: !ref <background_noise_strength>

attack_name: univ_noise
save_audio: True
load_audio: True

# Model information
model_label: tiny
model_name: !ref whisper-<model_label>
target_brain_class: !name:whisper_attack.sb_whisper_binding.WhisperASR
target_brain_hparams_file: !ref model_configs/<model_label>.yaml
source_model_name: !ref <model_name>
source_brain_class: !ref <target_brain_class>
source_brain_hparams_file: !ref model_configs/<model_label>.yaml

# Tokenizer information (compatible with target and source)
tokenizer_name: multilingual
tokenizer_builder: !name:whisper.tokenizer.get_tokenizer

output_folder: !ref <root>/attacks/<attack_name>/<source_model_name>
wer_file: !ref <output_folder>/wer.txt
save_folder: !ref <output_folder>
log: !ref <output_folder>/log.txt
save_audio_path: !ref <output_folder>/save
params_subfolder: 80_epochs
params_folder: !ref <output_folder>/<params_subfolder>

pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
    loadables: {}
    paths: {}
    collect_in: !ref <output_folder>
# pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
#    collect_in: !ref <output_folder>
#    loadables:
#       delta: !ref <delta>
#       epoch_counter: !ref <epoch_counter>
#       background_audio_path: !ref <background_audio_path>
#    paths:
#       delta: !ref <params_folder>/delta.ckpt
#       epoch_counter: !ref <params_folder>/epoch_counter.ckpt
#  -------------------------------------------------------------

dataset_prepare_fct: !name:robust_speech.data.librispeech.prepare_librispeech
dataio_prepare_fct: !name:robust_speech.data.dataio.dataio_prepare

# Data files
data_folder: !ref <root>/data/LibriSpeech # e.g, /localscratch/LibriSpeech
csv_folder: !ref <data_folder>/csv # e.g, /localscratch/LibriSpeech
# If RIRS_NOISES dir exists in /localscratch/xxx_corpus/RIRS_NOISES
# then data_folder_rirs should be /localscratch/xxx_corpus
# otherwise the dataset will automatically be downloaded
test_splits: ["test-clean"]
train_splits: ["train-clean-100"]
skip_prep: True
ckpt_interval_minutes: 15 # save checkpoint every N min
data_csv_name: test-clean
data_csv_name_train: train-clean-100
test_csv:
   - !ref <data_folder>/csv/<data_csv_name>.csv
train_csv: !ref <data_folder>/csv/<data_csv_name_train>.csv
batch_size: 1 # This works for 2x GPUs with 32GB
avoid_if_longer_than: 14.0
sorting: random

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
   checkpoints_dir: !ref <output_folder>
   recoverables:
      delta: !ref <delta>
      epoch_counter: !ref <epoch_counter>

# Feature parameters
sample_rate: 16000
n_fft: 400
n_mels: 80

# Decoding parameters (only for text_pipeline)
blank_index: 0
bos_index: 1
eos_index: 2

train_dataloader_opts:
    batch_size: !ref <batch_size>
test_dataloader_opts:
    batch_size: 1

logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <log>
    

